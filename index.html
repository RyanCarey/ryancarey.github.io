<link rel="stylesheet" href=style.css>

<p class="center-text"> [firstname].[lastname]@jesus.ox.ac.uk &middot;<br/>
<a href="ryan_carey_cv.pdf">CV</a> &middot; 
<a href="https://causalincentives.com/">Causal Incentives Working Group</a> &middot; <a href="https://scholar.google.com/citations?user=9U1CpcAAAAAJ&hl=en">Google Scholar</a> &middot; <a href="https://twitter.com/ryancareyai">Twitter</a></p>

<h1>Summary</h1>

<p>I’m a final-year PhD student at Oxford, supervised by <a href="https://www.stats.ox.ac.uk/~evans/">Robin Evans</a>, where I work on theoretical questions involving causal models. I’m also a cofounder of the <a href="http://causalincentives.com">Causal Incentives Working Group</a>, which uses causal models to reason about AI safety. Previously, I’ve been a research fellow at the Future of Humanity Institute, a research intern at DeepMind and OpenAI, and the founder of the <a href="https://forum.effectivealtruism.org/">EA Forum</a>.</p>

<p>I’ve worked on some theory for designing AI systems that are corrigible (that follow and don’t try to manipulate their instructions). Specifically, I’ve showed how reward learning systems <a href="https://arxiv.org/abs/1709.06275">may behave incorrigibly</a>, how corrigible systems <a href="https://arxiv.org/pdf/2305.19861">may behave unsafely</a>, and how <a href="https://arxiv.org/pdf/2305.19861">newer definitions</a>, if met, would guarantee safety.</p>

<p>I’ve also worked on <a href="https://arxiv.org/abs/2102.01685">characterising</a> and <a href="https://arxiv.org/abs/2204.10018">modifying</a> agents’ incentives, the former of which is closely related to the problem of <a href="https://arxiv.org/abs/2202.11629">identifying nonrequisite edges</a> in an influence diagram.</p>

<p>I’ve also worked on using machine learning to <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/predicting-judgments-tr2018.pdf">predict the outcomes of human deliberation</a>, and on analysing <a href="https://aiimpacts.org/interpreting-ai-compute-trends/">trends in AI experiments’ compute usage</a>.</p>

<h1>Selected Publications</h1>
<p><strong><a href="https://arxiv.org/abs/2305.19861">Human Control: Definitions and Algorithms</a></strong>: We study definitions of human control, including variants of corrigibility and alignment, the assurances they offer for human autonomy, and the algorithms that can be used to obtain them. <strong>Ryan Carey</strong>, Tom Everitt. UAI. 2023.</p>
<p><strong><a href="https://arxiv.org/abs/2301.02324">Reasoning about Causality in Games</a></strong>: Introduces (structural) causal games, a single modelling framework that allows for both causal and game-theoretic reasoning. Lewis Hammond, James Fox, Tom Everitt, <strong>Ryan Carey</strong>, Alessandro Abate, Michael Wooldridge. Artificial Intelligence Journal, 2023.</p>
<p><strong><a href="https://arxiv.org/abs/2204.10018">Path-Specific Objectives for Safer Agent Incentives</a></strong>: How do you tell an ML system to optimize an objective, but not by any means? E.g. optimize user engagement without manipulating the user? Sebastian Farquhar, <strong>Ryan Carey</strong>, Tom Everitt. AAAI. 2022.</p>
<p><strong><a href="https://arxiv.org/abs/2202.11629">A Complete Criterion for Value of Information in Soluble Influence Diagrams</a></strong>: Presents a complete graphical criterion for value of information in influence diagrams with more than one decision node, along with ID homomorphisms and trees of systems. Chris van Merwijk*, <strong>Ryan Carey</strong>*, Tom Everitt AAAI. 2022.</p>
<p><strong><a href="https://arxiv.org/abs/2202.10816">Why Fair Labels Can Yield Unfair Predictions: Graphical Conditions for Introduced Unfairness</a></strong>: When is unfairness incentivized? Perhaps surprisingly, unfairness can be incentivized even when labels are completely fair. Carolyn Ashurst, <strong>Ryan Carey</strong>, Silvia Chiappa, Tom Everitt. AAAI. 2022.</p>
<p><strong><a href="https://arxiv.org/abs/2102.01685">Agent Incentives: A Causal Perspective</a></strong>: An agent’s incentives are largely determined by its causal context. This paper gives sound and complete graphical criteria for four incentive concepts: value of information, value of control, response incentives, and control incentives. Tom Everitt*, <strong>Ryan Carey</strong>*, Eric Langlois*, Pedro A. Ortega, Shane Legg. AAAI. 2021.</p>




<h1>Other Writing</h1>
<ul>
    <li><a href="EA_Handbook.pdf">EA Handbook</a> (editor)</li>
    <li><a href="https://aiimpacts.org/interpreting-ai-compute-trends/">Interpreting AI compute trends</a></li>
    <li><a href="https://forum.effectivealtruism.org/posts/qYbqX3jX4JnTtHA5f/leverage-research-reviewing-the-basic-facts">Leverage Research: reviewing the basic facts</a></li>
    <li><a href="https://forum.effectivealtruism.org/posts/EP6X362Q3ziibA99e/show-a-framework-for-shaping-your-talent-for-direct-work">SHOW: A framework for shaping your talent for direct work</a></li>
    <li><a href="https://80000hours.org/2014/06/the-payoff-and-probability-of-obtaining-venture-capital/">The payoff and probability of obtaining venture capital</a></li>
    <li><a href="https://80000hours.org/2014/05/how-much-do-y-combinator-founders-earn/">How much do Y Combinator founders earn?</a></li>
</ul>
